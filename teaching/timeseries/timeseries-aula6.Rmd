---
title: Séries de Tempo
subtitle: Aula 6 - Outros modelos univariados
author: Regis A. Ely
institute: |
    | Departamento de Economia
    | Universidade Federal de Pelotas
date: "`r format(Sys.time(), '%d de %B de %Y')`"
fontsize: 12pt
output:
 beamer_presentation:
    template: ~/Dropbox/resources/templates/latex/latex-beamer.tex
    latex_engine: pdflatex
    toc: true
    fig_caption: true
    slide_level: 3
make149: true
---

# Outros modelos univariados

Alguns outros modelos univariados que podemos utilizar para previsão incluem:

- **Média**: a previsão de um valor futuro é sempre igual a média dos valores passados
- **Naive**: a previsão de um valor futuro é o valor imediatamente anterior (passeio aleatório)
- **Seasonal Naive**: a previsão de um valor futuro é o valor sazonal imediatamente anterior
- **Modelos dinâmicos aditivos**: modelos que incluem componentes sazonais, tendências e regressores exógenos
  
# Identificação e seleção de modelos

- Em modelos ARIMA, usamos os critérios de identificação para selecionar entre modelos distintos

- Como estes critérios dependem do valor da função de verossimilhança, não podemos utilizá-los para comparar modelos com estruturas distintas^[Também deve-se utilizar com cautela o critério AICc quando o horizonte de previsão é longo.]

- Nesse caso, devemos construir uma estratégia de validação das previsões e utilizar medidas de acurácia baseadas nos erros de previsão

- Pode-se demonstrar que o critério de Akaike corrigido é equivalente a uma validação do tipo *leave-one-out* (LOOCV)

# Validação

- Para avaliarmos a qualidade da previsão de um modelo, devemos construir uma estratégia de validação em treinamos o modelo em um pedaço dos dados (*conjunto treino*) e testamos a previsão em outro pedaço (*conjunto teste*)

- Existem diversas estratégias de validação possíveis, sendo que a escolha da estratégia depende do problema analisado

- As estratégia mais comum de validação para séries de tempo é a *rolling cross validation*

## Rolling cross validation

- Antes de construirmos a estratégia de validação, podemos separar os dados em dois grupos (treino e teste), utilizando, por exemplo, 75% dos dados para o grupo treino e 25% para o grupo teste

- A estratégia *rolling cross validation* se aplica no conjunto treino, e define um conjunto inicial de observações ($n$), uma janela móvel ($j$) e um horizonte de previsão ($h$)
    - Primeiro estimamos o modelo nas $n$ observações iniciais,
    - Depois avaliamos a previsão de horizonte $h$,
    - Depois estimamos o modelo novamente adicionando as $j$ próximas observações,
    - Seguimos estas etapas até o fim da série de tempo

# Medidas de performance

As medidas mais comuns para calcular a performance de modelos de previsão incluem:

- **Erro absoluto médio**: $MAE = \frac{\sum_{t=1}^T | \hat{Y_t} - Y_t |}{T}$
- **Raiz do erro quadrático médio**: $RMSE = \sqrt{\frac{\sum_{t=1}^T (\hat{Y_t} - Y_t)^2}{T}}$
- **Erro percentual médio absoluto**: $MAPE = \frac{1}{T} \overset{T}{\underset{t=1}{\sum}} | \frac{Y_t - \hat{Y_t}}{Y_t} |$

# Exemplo no R

Para este exemplo iremos utilizar os pacotes e os dados da aula passada:

```{r pacotes, eval=TRUE, echo=TRUE, message=FALSE}
library(tidyverse)
library(lubridate)
library(tsibble)
library(feasts)
library(fable)
library(fasster)
data <- tourism %>%
  group_by(Purpose) %>%
  summarise(Trips = sum(Trips))
```

## Conjunto treino

Para construir um procedimento de validação, vamos primeiro definir o horizonte de previsão $h$ e então criar um conjunto treino excluindo as últimas $h$ observações para medirmos a performance da previsão (conjunto teste):

```{r horizon, eval=TRUE, echo=TRUE}
h <- 10
train <- data %>%
  group_by(Purpose) %>%
  slice(1:(n()-h)) %>%
  ungroup()
```

## Conjunto validação

A função `stretch_tsibble` possibilita criar conjuntos de validação utilizando janelas móveis de 10 em 10, começando com 20 observações:

```{r val_set, eval=TRUE, echo=TRUE}
train_cv <- train %>%
  group_by(Purpose) %>%
  slice(1:(n() - 10)) %>%
  stretch_tsibble(.init = 20, .step = h) %>%
  ungroup()
```

Uma nova coluna `.id` será criada com o identificador do conjunto validação. Ao todo são 60 observações por série de tempo (10 foram inicialmente excluídas), o que nos dá 5 grupos distintos (o primeiro grupo contém 20, o segundo 30, etc.)

## Estimação

Vamos criar uma função para estimar vários modelos em apenas um comando:

```{r models, eval=TRUE, echo=TRUE}
models <- function(data) {
  data %>%
    model(
      MEAN = MEAN(log(Trips)),
      NAIVE = NAIVE(log(Trips)),
      SNAIVE = SNAIVE(log(Trips)),
      ETS = ETS(log(Trips)),
      ARIMA = ARIMA(log(Trips)),
      DLM = FASSTER(log(Trips) ~ poly(1) + season(4))
    )
}
```

### Estimação

Agora vamos estimar todos os modelos no conjunto treino e nos grupos do conjunto validação, realizando as previsões com a função `forecast`:

```{r fit, eval=TRUE, echo=TRUE}
train_fit <- models(train)
train_cv_fit <- models(train_cv)
train_fc <- forecast(train_fit, h = 10)
cv_fc <- forecast(train_cv_fit, h = 10)
```

## Performance das previsões

Com a função `accuracy` podemos calcular a performance das previsões no conjunto treino e validação:

```{r accuracy, eval=TRUE, echo=TRUE}
acc_train <- accuracy(train_fit)
acc_cv <- cv_fc %>% accuracy(train)
```

Minimizar o erro de previsão no conjunto treino pode levar a `overfitting`, por isso precisamos olhar para os erros de previsão no conjunto validação e no conjunto teste

### Performance das previsões

\tiny
```{r accuracy2, eval=TRUE, echo=TRUE}
acc_train %>% print(n = 24)
```

### Performance das previsões

\tiny
```{r val_acc, eval=TRUE, echo=TRUE}
acc_cv %>% arrange(Purpose) %>% print(n = 24)
```

## Seleção do melhor modelo

Os melhores modelos devem ser aqueles que minimizam o erro de previsão no conjunto validação. Iremos utilizar o `RMSE` para escolher:

\footnotesize
```{r best_model, eval=TRUE, echo=TRUE}
best_model <- acc_cv %>%
  group_by(Purpose) %>%
  filter(RMSE == min(RMSE))
best_model
```

### Seleção do melhor modelo

Agora podemos estimar os melhores modelos no conjunto treino completo e calcular as previsões:

```{r best_fit, eval=TRUE, echo=TRUE}
good_models <- train %>%
  model(
    ETS = ETS(log(Trips)),
    DLM = FASSTER(log(Trips) ~ poly(1) + season(4))
  )
train_fc <- forecast(good_models, h = h)
```

### Seleção do melhor modelo

Por fim, podemos avaliar a performance do melhor modelo no conjunto teste que contém 10 observações não utilizadas durante todo o procedimento:

\scriptsize
```{r fc_best, eval=TRUE, echo=TRUE}
train_fc %>% accuracy(data) %>% arrange(Purpose)
```

### Seleção do melhor modelo

Podemos comparar graficamente as previsões para estas 10 observações com os dados verdadeiros através da função `autoplot`:

```{r fc_plot1, eval=FALSE, echo=TRUE}
train_fc %>% autoplot(data)
```

Observa-se que alguns modelos parecem não estar captando bem a tendência de crescimento mais recente nas séries de tempo. Como você corrigiria isso?

### Seleção do melhor modelo

```{r fc_plot2, eval=TRUE, echo=FALSE}
train_fc %>% autoplot(data)
```

## Previsões finais

Por fim, podemos estimar os melhores modelos no conjunto total de dados para fazer previsões para períodos desconhecidos:

```{r fit_full, eval=TRUE, echo=TRUE}
final_models <- data %>%
  model(
    ETS = ETS(log(Trips)),
    DLM = FASSTER(log(Trips) ~ poly(1) + season(4))
  )
final_fc <- forecast(final_models, h = h)
```

E plotar as previsões finais:

```{r fc_final1, eval=FALSE, echo=TRUE}
final_fc %>% autoplot(data)
```

### Previsões finais

```{r fc_final2, eval=TRUE, echo=FALSE}
final_fc %>% autoplot(data)
```

### Previsões finais

Como podemos melhorar as previsões destes modelos?

- Considerar diferentes estratégias de validação (divisão entre conjuntos treino, teste e validação)

- Utilizar parâmetros diferentes nas estimações dos modelos

- Estimar outros modelos e tirar a média de previsões de modelos distintos

- Utilizar outras medidas de performance
